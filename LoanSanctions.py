# -*- coding: utf-8 -*-
"""Quiz4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OgkD4T9_bpnZoxShCnPfq8eVIU-soEtb
"""

import pandas as pd

df = pd.read_csv("/loan_sanction.csv")

import seaborn as sns

import numpy as np
import matplotlib.pyplot as plt

print(df.info())

print(df.isnull().sum())

print(df.mean())

print(df.hist())

print(df)

#Filled empty credit history fields with the average of 0.84

df.Credit_History = df.Credit_History.fillna(0.84)

#Filled in empty loan amounts wit the average amount of $146.41

df.LoanAmount = df.LoanAmount.fillna(146.41)

#Filled self employed with no because that was the vast majority.

df.Self_Employed = df.Self_Employed.fillna("No")

#Filled the loan amount term with the average of 342

df.Loan_Amount_Term = df.Loan_Amount_Term.fillna(342)

df.dropna(inplace = True)

print(df.hist())

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

#Decision tree classifier
def decision_tree_classifier(train_x, train_y):
  from sklearn import tree
  model = tree.DecisionTreeClassifier()
  model.fit(train_x, train_y)
  return model

#KNN Classifier
def knn_classifier(train_x, train_y):
  from sklearn.neighbors import KNeighborsClassifier
  model = KNeighborsClassifier()
  model.fit(train_x,train_y)
  return model

#Naive Bayes Classifier
def naive_bayes_classifier(train_x, train_y):
  from sklearn.naive_bayes import MultinomialNB
  model = MultinomialNB(alpha = 0.01)
  model.fit(train_x, train_y)
  return model

#Logistic Regression Classifier
def logistic_regression_classifier(train_x, train_y):
  from sklearn.linear_model import LogisticRegression
  model = LogisticRegression(penalty = 'l2')
  model.fit(train_x, train_y)
  return model

#Gradient Boosting Classifier
def Gradient_Boosting_Classifier(train_x, train_y):
  from sklearn.ensemble import GradientBoostingClassifier
  model = GradientBoostingClassifier()
  model.fit(train_x, train_y)
  return model

#SVM Classifier
def svm_classifier(train_x, train_y):
  from sklearn.svm import SVC
  model = SVC(kernel = 'rbf', probability= True)
  model.fit(train_x, train_y)
  return model

# Random Forest Classifier
def random_forest_classifier(train_x, train_y):
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier(n_estimators= 100)
  model.fit(train_x, train_y)
  return model

#import for ANN
from tensorflow import keras
from tensorflow.keras import layers

from sklearn import preprocessing

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout, BatchNormalization
from tensorflow.keras import optimizers
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from keras import callbacks
from keras.optimizers import Adam

y_train[:100]

df = df.drop('Loan_ID', axis = 1)

print(df.info())

print(df)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Gender = le.fit_transform(df.Gender)
Gender
df.Gender = Gender

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Married = le.fit_transform(df.Married)
Married
df.Married = Married

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Dependents = le.fit_transform(df.Dependents)
Dependents
df.Dependents = Dependents

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Education = le.fit_transform(df.Education)
Education
df.Education = Education

from pandas.core.algorithms import SelectNFrame
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Self_Employed = le.fit_transform(df.Self_Employed)
Self_Employed
df.Self_Employed = Self_Employed

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Property_Area = le.fit_transform(df.Property_Area)
Property_Area
df.Property_Area = Property_Area

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Loan_Status = le.fit_transform(df.Loan_Status)
Loan_Status
df.Loan_Status = Loan_Status

#Splitting training and test set
x=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state = 42)

print(df.head)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

test_classifiers = [
    'KNN',
    'DT',
    'NB',
    'LR',
    'RF',
    'SVM',
    'GBDT'
]
classifiers = {
    'KNN':knn_classifier,
    'DT':decision_tree_classifier,
    'NB':naive_bayes_classifier,
    'LR':logistic_regression_classifier,
    'RF':random_forest_classifier,
    'SVM':svm_classifier,
    'GBDT':Gradient_Boosting_Classifier

}

print(X_train)

y_train

for classifier in test_classifiers:
  print('***************** %s *************' % classifier)

  model = classifiers[classifier](X_train, y_train)

  predict = model.predict(X_test)
  train_out = model.predict(X_train)

  matrix = confusion_matrix(y_test, predict)
  print(matrix)
  class_report = classification_report(y_test, predict)
  print(class_report)

from sklearn.multioutput import MultiOutputClassifier
from sklearn.linear_model import LogisticRegression

# Early stopping for ANN
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,  #Minimum amount of change to count as improvement
    patience = 20,  # how many epochs to wait before stopping
    restore_best_weights=True
)

#initializing the NN
model = Sequential()

#layers

model.add(Dense(units = 11, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11 ))

model.add(Dropout(0.25))
model.add(Dense(units= 8, kernel_initializer='uniform', activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

#Compiling the ANN
opt = Adam(learning_rate = 0.00009)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])

# Train the ANN
history = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks = [early_stopping], validation_split = 0.2)

# Plotting training and validation loss

history_df = pd.DataFrame(history.history)
plt.plot(history_df.loc[:,['loss']], "#BDE2E2", label = "Training Loss")
plt.plot(history_df.loc[:, ['val_loss']], "#C2C4E2", label = 'Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc="best")

plt.show()

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)
matrix = confusion_matrix(y_test,y_pred)
print(matrix)
class_report = classification_report(y_test, y_pred)
print(class_report)